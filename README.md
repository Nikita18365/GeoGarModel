# GeoGarModel
Software package for geological and geophysical modeling of ground and aerial survey data

# Главное окно программы:
Главное окно программы представлено пространством для визуализации грид файлов регулярной и нерегулярной сети формата *zmap. Пример файла формата *.zmap можно получить путем экспорта грид файла в формат *.zmap в программном обеспечении Surfer 16. Примеры файлов Zmap также представлены в главном дистрибутиве программы.
В правой части располагается список загруженных файлов формата *.zmap. При нажатии на соответствующий файл будет происходить его визуализация в левом окошке со стандартными элементами оформления, а именно:
 - цветовая шкала rainbow
- гистограммное цветовое распределение
- без названия карты
- без подписи цветовой шкалы
Под списком загруженных файлов располагаются элементы изменения оформления карты, которые включают в себя:
- Минимальное значение цветового отображения
- Максимальное значение цветового отображения

 Под списком загруженных файлов располагаются элементы изменения оформления карты, которые включают в себя:
 
- Цветовая шкала:
1) hsv
2) bwr
3) colorwarm
4) gist_rainbow
5) ocean
6) rainbow
7) seismic
8) tab20c
9) terrain
10) twilight
11) twilight_shifted
![image](https://github.com/user-attachments/assets/35c4becd-260b-4b18-a758-65778f58f4cf)
- Название карты
- Подпись цветовой шкалы (справа от цветовой шкалы; rotate = 90)

Под элементами оформления карты находятся 3 кнопки:

1)	«Построить точки обучающей и тестовой выборки». После загрузки обучающего СС датасета, точки обучающей и контрольной (валидационной) выборок можно визуализировать на карте. !Система координат (СК) отображающегося *.zmap файла должна совпадать с СК обучающего датасета для корректной визуализации.
2)	«Добавить shp файл». После загрузки одного или нескольких файлов формата *.shp их можно визуализировать на карте. !Система координат (СК) отображающегося *.zmap файла должна совпадать с СК файла формата *.shp для корректной визуализации.
3)	Кнопка «ОК». При нажатии данной кнопки для уже отображающегося файла формата *.zmap обновляются его элементы оформления. !При этом пункты 1 и 2 полностью удаляются с карты, но сами файлы сохраняются в корне программы и повторная их загрузка не требуется.
# Элементы зума:
В верхней части программы располагается следующая иконка:

 ![image](https://github.com/user-attachments/assets/30b68ec6-c35e-4d2e-b108-f9e7cc04e97d)
 
При нажатии левой кнопкой мыши на данную иконку происходит возврат к стандартному разрешению с возвратом всех элементов, которые были на карте в момент увеличения зума.
Также пользователь может приблизить и отдалить карту. Приближение осуществляется за счет нажатия на следующую кнопку в элементе тулбара:

 ![image](https://github.com/user-attachments/assets/22e015b8-e413-4762-84f5-a1de44880ace)
 
Для отдаления используется иконка:

 ![image](https://github.com/user-attachments/assets/b222a203-e9db-4ff1-8507-b0aaba33518c)
 
Пользователю достаточно нажать 1 раз левой кнопкой мыши в любой области карты для уменьшения масштаба карты на 10% по обоим осям.
# Элемент меню – «Загрузка данных»

1) Загрузить обучающую СС выборку

Данная кнопка реализует загрузку одного файла обучающей выборки (обучающийся участок) формата *.xlsx.
Пример входного файла:

![image](https://github.com/user-attachments/assets/f46fc002-e967-458f-9caf-2e341b388d60)

Первые 2 столбца обязательно должны содержать в себе координаты атрибутов (X, Y).

Далее, начиная с 3 столбца идут признаки, по которым будет происходить обучение.

Кодировка xlsx или xls файла – обязательно UTF8.

Последний столбец является целевой переменной, которая будет предсказываться.

2) Загрузить прогнозную выборку

Данная кнопка реализует загрузку одного файла прогнозной выборки (прогнозный участок) формата *.xlsx.
Файл ничем не отличается от файла обучающей СС выборки за исключением последнего столбца. Для прогнозной выборки целевая переменная неизвестна. Если кол-во столбцов обучающей СС выборки 15, то прогнозной должно быть 14.
- Загрузить shp файл
Данная кнопка реализует загрузку одного или нескольких файлов формата *.shp. !В папке с файлом формата *.shp должны находиться сцепленные с ним файлы следующих форматов:
- *.dbf
- *.prj
- *.sbn
- *.shx

Все названия добавленных *.shp файлов находятся в главном окне программы. При нажатии левой кнопкой мыши пользователь может поменять цвет и толщину линии. Директория расположения shp файла может быть любой (не обязательно в дистрибутиве программы)
Для очистки карты от *.shp файлов достаточно нажать кнопку «ОК» в главном окне программы. Если пользователь захочет визуализировать *.shp файлы снова, он может это сделать также нажав кнопку «Добавить shp файл». Файлы визуализируются с тем же цветом и той же толщиной линии. Пример:

![image](https://github.com/user-attachments/assets/e6cc00c7-c922-428b-a435-0ef482137c52)

3) Загрузить Zmap файл

Данная кнопка реализует загрузку одного или нескольких файлов формата *.zmap. Файлы Zmap должны располагаться в главном дистрибутиве программы, без подпапок.
# Элемент меню – «Просмотр данных»
1) Просмотр данных СС выборки

Данная функция реализует просмотр и редактирование данных обучающей СС выборки. Пример визуализации:

![image](https://github.com/user-attachments/assets/2e78cd49-600a-4a47-8616-34b0a1f2acd2)

2) Просмотр данных прогнозной выборки

Данная функция реализует просмотр и редактирование данных прогнозной выборки.
# Элемент меню – «Визуализация данных»
- Визуализация данных СС выборки

После загрузки СС выборки пользователь может визуализировать её.

- Визуализация данных прогнозной выборки

После загрузки прогнозной выборки пользователь может визуализировать её.

- Визуализация y_pred

После этапа подбора оптимальных гиперпараметров пользователь может визуализировать предсказанные значения (1), данные целевой переменной train выборки (2) и разность предсказанных значений с test выборкой (3). Под графиками располагается 3 кнопки при помощи которых пользователь может экспортировать нужную ему карту в формате *.txt. Первый и второй столбец файла отвечают координатам (X,Y), а третий столбец экспортируемой целевой переменной.

![image](https://github.com/user-attachments/assets/04e6a903-69ac-4885-b9e4-be7063103bf9)

# Элемент меню – «Статистические характеристики данных»
- Статистические характеристики данных СС выборки
- Статистические характеристики данных прогнозной выборки
После загрузки данных обучающей СС выборки пользователь может проанализировать основные статистические хар-ки всех атрибутов, такие как:
- Среднее значение
- Стандартное отклонение
- Минимум
- Максимум
- Квантиль 25%
- Квантиль 75%
Пример:

![image](https://github.com/user-attachments/assets/10dbabd9-96fc-423d-b9ae-6e7bf75d2354)

# Shapely Plot
Математическая основа расчета Shapley значения:
Shapely values в теории игр:
Теория игр - это область математики, изучающей взаимодействие (игру) между игроками, преследующими некие цели и действующими по неким правилам. Кооперативной игрой называется такая игра, в которых группа игроков (коалиция) действует совместно. С середины XX века (Shapley, 1952) известны так называмые Shapley values, которые позволяют численно оценить вклад каждого игрока в достижение общей цели.
Примечание. Понятие "игра" в данном случае может ввести в заблуждение. Обычно под игрой понимается противостояние двух и более сторон, здесь же речь пойдет скорее о кооперативном процессе, в котором каждый участник вносит вклад в общий результат.
Пусть существует характеристическая функция υ, которая каждому множеству игроков сопоставляет число - эффективность данной коалиции игроков, действующей совместно. Тогда Shapley value для каждого игрока - это число, рассчитываемое по достаточно простой формуле. Обозначим за за Δ(i,S) прирост эффективности от добавления игрока i в коалицию игроков S:

![image](https://github.com/user-attachments/assets/b60d5058-e70e-4745-804d-cc2d1d6ce6f8)

Пусть всего есть N игроков. Рассмотрим множество П всех возможных упорядочиваний игроков, и обозначим за (players before i in π) множество игроков, стоящих перед игроков i в упорядочивании π. Shapley value для игрока i рассчитывается таким образом:

![image](https://github.com/user-attachments/assets/edab8c83-1f80-4bbd-b2b9-2e9413a5f619)

То есть мы считаем средний прирост эффективности от добавления i-го игрока в коалицию игроков, стоящих перед ним, по всем возможным упорядочиваниям игроков (количество элементов суммы равно N!).
Формула выше задается аксиоматически, то есть формулируется ряд необходимых свойств и доказывается, что данное решение является единственным, которое им удовлетворяет. Поскольку Δ(i,S) не зависит от порядка игроков в S, то можно объединить равные друг другу слагаемые и переписать формулу в следующем эквивалентном виде:

![image](https://github.com/user-attachments/assets/91e76bdf-1f05-4c46-859e-a6babe3e4331)

Формула является взвешенной суммой по всем подмножествам игроков, не содержащих игрока i, в которой веса принимают наибольшие значения при |S|≈0 или |S|≈ |N| и наименьшие значения при |S|≈ |N|/2.

1) Bar Shapely Plot

При нажатии на кнопку Bar Shapely Plot создает гистограмму из набора значений SHAP для тренировочной выборки.
Пример:

![image](https://github.com/user-attachments/assets/5f8d560a-ef79-4b61-8004-d7fbb13d10a1)

На гистограмме представлены средние значения Shap Value для 13 признаков. Схема читается снизу вверх, и признаки упорядочены по возрастанию их SHAP values. Например, SHAP value для признака Grav_Trend3 (имеющего значение -19.5) говорит о том, что данный атрибут уменьшает величину предсказания модели, по сравнению с отсутствием этого признака, при произвольном наличии других признаков. Таким же образом себя ведет и GravTrend1, Ostat_GravMagInv, GravReg10km, GravReg2km.

2) Beeswarm Shapely Plot

График «basewarm plot» предназначен для отображения подробной информации о том, как основные характеристики набора данных влияют на результаты модели. Каждый экземпляр, для которого дано объяснение, представлен одной точкой на каждой оси. Положение точки по оси X определяется значением SHAP (shap_values.value[instance,feature]) для этой характеристики, и точки «нагромождаются» вдоль каждой строки характеристик, чтобы показать плотность. Цвет используется для отображения исходного значения характеристики (shap_values.data[instance,feature]). На графике ниже мы видим, что пониженные значения региональной составляющей гравитационного поля и глубины границы Мохоровичича имеют повышенное влияние в модели, по сравнению с другими признаками.

![image](https://github.com/user-attachments/assets/ffd9e35b-974c-4b4c-b840-db3ef2ff7ad9)

3) Heatmap Shapely Plot

При передаче матрицы значений SHAP в функцию построения тепловой карты создаётся график с примерами по оси X, входными данными модели по оси Y и значениями SHAP, закодированными в цветовой шкале. По умолчанию образцы упорядочиваются с помощью shap.order.hclust, которая упорядочивает образцы на основе иерархической кластеризации по сходству их объяснений. В результате образцы с одинаковым результатом модели по одной и той же причине группируются вместе.
Выходные данные модели отображаются над матрицей тепловой карты (в центре которой находится .base_value), а глобальная важность каждого входного параметра модели отображается в виде гистограммы в правой части графика (по умолчанию это shap.order.abs.mean показатель общей важности).

![image](https://github.com/user-attachments/assets/c862ad8d-0380-42a8-9aea-120c2aec80d1)
# Элемент меню – «Train_Test»
Данный алгоритм реализует разделение данных обучающего датасета на обучающую и контрольные (тестовая) выборки методом train_test_split из библиотеки scikitlearn. В следующем всплывающем окне пользователь должен ввести число в %, которое будет определять, сколько процентов тестовой выборки использовать:

![image](https://github.com/user-attachments/assets/8a4c3b68-cc09-427b-a930-ed580427b510)

После нажатия кнопки ОК пользователь получит размеры train и test выборок в формате (строки, столбцы):

![image](https://github.com/user-attachments/assets/930895f4-c5d1-44d9-8be8-85788af907e3)

# Элемент меню – «Machine Learning»
Данный элемент меню включает в себя следующие функции:
- Нормализация данных:
Для удаления средних значений и получения единичной дисперсии в данных используется метод StandadScaler из библиотеки scikitlearn.  Данный процесс гарантирует, что все атрибуты будут находятся в одинаковом масштабе, предотвращая доминирование какого-либо атрибута в процессе обучения из-за его большей величины.
Преобразование, выполняемое StandardScaler, можно выразить математически следующим образом:

z = x − μ/ σ

где x представляет собой исходное значение признака, μ — среднее значение признака, σ — стандартное отклонение, а z — стандартизированное значение признака.
После нажатия данной кнопки пользователь получает всплывающее окно, в котором указано, что данные нормализованы.

![image](https://github.com/user-attachments/assets/702a165c-d51e-4399-971b-3d663eb2c7dc)

Нормализация осуществляется как для данных обучающей, так и для данных тестовой выборок.
Метод обучается на тренировочных train данных, к которым применяется функция fit_transform. Для данных test выборки применяется обычная функция transform. Аналогичная методика осуществляется и для получения значений целевой переменной на прогнозном участке. Обучение метода происходит на всех данных обучающего участка, а для прогнозного участка используется метод transform.

# XGBoost (XGBRegressor v.2.1.1):
В данной функции реализован процесс обучения, предсказания и подбора оптимальных гиперпараметров модели градиентного бустинга библиотеки XGBoost на обучающем датасете. При запуске функции пользователю предлагается выбрать диапазоны, в пределах которых будут определяться оптимальные гиперпараметры. Среди самых оптимальных алгоритмов поиска гиперпараметров можно выделить следующие – RandomizedSearchCV (случайный поиск), GridSearchCV (поиск по решетке) и Hyperopt.
Поиск по решётке. В этом способе значения гиперпараметров задаются вручную, затем выполняется их полный перебор. Популярной реализацией этого метода является Grid Search из sklearn. Несмотря на свою простоту этот метод имеет и серьёзные недостатки:
Очень медленный т.к. надо перебрать все комбинации всех параметров. Притом перебор будет продолжаться даже при заведомо неудачных комбинациях.

Часто в целях экономии времени приходится укрупнять шаг перебора, что может привести к тому, что оптимальное значение параметра не будет найдено. Например, если задан диапазон значений от 100 до 1000 с шагом 100 (примером такого параметра может быть количество деревьев в случайном лесе, или градиентном бустинге), а оптимум находится около 550, то GridSearch его не найдёт.

Случайный поиск. Здесь параметры берутся случайным образом из выборки с указанным распределением. В sklearn этот метод реализован как Randomized Search. В большинстве случаев он быстрее GridSearch, к тому же значения параметров не ограничены сеткой. Однако, даже это не всегда позволяет найти оптимум и не защищает от перебора заведомо неудачных комбинаций.

В данной функции, также как и во всех остальных функциях, реализован метод hyperopt – популярной python-библиотеки для подбора гиперпараметров. В ней реализовано 3 алгоритма оптимизации: классический Random Search, метод байесовской оптимизации Tree of Parzen Estimators (TPE), и Simulated Annealing – метод имитации отжига. Hyperopt может работать с разными типами гиперпараметров –непрерывными, дискретными, категориальными и т.д, что является важным преимуществом этой библиотеки. Важным плюсом метода является то, что он сочетает в себе 2 важных качества – это малая времязатратность и отличная точность подобранных гиперпараметров, благодаря применению высокоточных методов оптимизации.

Применение метода hyperopt можно посмотреть в статье по ссылке: https://habr.com/ru/articles/542624/

Окно выбора диапазона изменения гиперпараметров для booster=gbtree выглядит следующим образом:

![image](https://github.com/user-attachments/assets/79749d38-fe84-4e4f-beb7-1f00348eb0cd)

Окно выбора диапазона изменения гиперпараметров для booster=gblinear выглядит следующим образом:

![image](https://github.com/user-attachments/assets/2a1acbc5-7899-4349-b549-d280e1e5cbab)

После нажатия кнопки «ОК» пользователю отображается список пояснений и ошибок:

![image](https://github.com/user-attachments/assets/43204a8d-d506-40a9-aad6-01f587e08052)

Если пользователь ввел интервал, который не укладывается в допустимые пределы, то в окне выше мы получаем ошибку и процедуру следует переделать заново, даже, если на выходе мы получаем график лосс функции. В данном случае для гиперпараметра max_depth был введен интервал от (-1) до 20.

Если пользователь оставил место для ввода гиперпараметра пустым (и минимальное, и максимальное значение, и шаг или максимальное и минимальное значение), тогда в данной консоли выводится сообщение «Пояснение». Стоит обращать внимание только на Ошибки. Если в консоли пользователь увидел ошибки, то ему следует нажать кнопку «ОК» и ввести значения повторно, в противном случае алгоритм не будет запускаться.

Для параметров типа int используется поиск методом hp.quniform = round(uniform(low, high) / q) * q, где q - это целое число определяющее дискретизацию.

Для параметров типа float используется поиск методом hp.uniform.

hp.uniform — этот метод принимает два параметра — нижнее и верхнее значения, определяющие диапазон, из которого мы хотим выбрать различные значения. Он извлекает значения из равномерного распределения.

Для гиперпараметров регуляризации (reg_alpha, reg_lambda) и шага обучения (learning_rate) используется поиск по логнормальному распределению при помощи функции hp.loguniform(‘name_param’, min*np.log(10), max*np.log(10)). Пользователем должны быть введены только числа min и max. Данные числа могут быть как отрицательные и положительные, так и 0. Например, если пользователем было введено min=(-4), а max=2, тогда будет получено логнормальное распределение параметра из диапазона (10^(-4); 10^2) и на выходе в диалоговом окне пользователь получит оптимальное значение гиперпараметра типа float при заданных условиях. Если пользователем ничего не было введено, то используются значения по умолчанию.

Если пользователь не ввел мин. и макс. значение (+шаг) гиперпараметра, то значение гиперпараметра используется по умолчанию. Значения гиперпараметров по умолчанию будет представлено после итогового расчета.
Пользователю предлагается выбрать диапазоны поиска следующих гиперпараметров:

Общие параметры:
1)	**booster [по умолчанию= gbtree]**

Какой усилитель использовать. Можно использовать gbtree или gblinear; gbtree используют модели на основе деревьев, а gblinear использует линейные функции с применением регуляризирующих алгоритмов. Реализация бустера dart будет представлена позже. 

Важным моментом является выбор в качестве бустера gblinear, так как для него используются другие гиперпараметры. Для данного бустера пользователь должен заполнить только диапазоны изменения следующих гиперпараметров – reg_alpha, reg_lambda, updater, feature_selector, top_k. В случае отсутствия ввода одного из данных гиперпараметров, они будут использоваться по умолчанию. Остальные гиперпараметры не будут учитываться при подборе и их значение будет установлено на «по умолчанию».

2)	**device** [по умолчанию= cpu]

Устройство для запуска XGBoost. Пользователь может установить одно из следующих значений:

cpu:Использует центральный процессор.

cuda:Использует графический процессор (устройство CUDA).

Пользователь может ввести либо cpu, либо cuda. При этом, если пользователь хочет использовать версию на графическом процессоре, то ему потребуется наличие поддержки видеокарты технологии CUDA, а также вручную установить на своем ПК версию 2.1.1 XGBoost для графического процессора.

**Параметры для booster = gbtree:**

1)	**eta** [по умолчанию=0.3, псевдоним: learning_rate]

Уменьшение размера шага, используемое при обновлении для предотвращения переобучения. После каждого этапа повышения точности мы можем напрямую получить веса новых признаков, а eta уменьшает веса признаков, чтобы сделать процесс повышения точности более консервативным.

диапазон: [0,1]

2)	**gamma** [по умолчанию=0, псевдоним: min_split_loss]

Минимальное снижение потерь, необходимое для дальнейшего разделения на листовом узле дерева. Чем больше gamma, тем более консервативным будет алгоритм. Обратите внимание, что дерево, в котором не было произведено никаких разделений, всё равно может содержать один конечный узел с ненулевым показателем.

диапазон: [0,∞]

3)	**max_depth** [по умолчанию=6]

Максимальная глубина дерева. Увеличение этого значения сделает модель более сложной и повысит вероятность переобучения. Значение 0 указывает на отсутствие ограничений по глубине. Имейте в виду, что XGBoost активно расходует память при обучении глубокого дерева. exact Метод дерева требует ненулевого значения.

диапазон: [0,∞]

4)	**n_estimators** (тип данных - [int])

Количество деревьев, усиленных градиентом. Эквивалентно количеству раундов усиления.

По умолчанию 1

диапазон: (0,∞]

5)	**min_child_weight** [по умолчанию=1]

Минимальная сумма весов экземпляров (гессиан), необходимых для дочернего элемента. Если на этапе разбиения дерева получается листовой узел с суммой весов экземпляров меньше min_child_weight, то процесс построения прекращает дальнейшее разбиение. В задаче линейной регрессии это просто соответствует минимальному количеству экземпляров, необходимых для каждого узла. Чем больше min_child_weight, тем консервативнее будет алгоритм.

диапазон: [0,∞]

6)	**subsample** [по умолчанию=1]

Коэффициент подвыборки обучающих примеров. Если установить его равным 0,5, XGBoost будет случайным образом выбирать половину обучающих данных перед построением деревьев. Это предотвратит переобучение. Подвыборка будет происходить один раз на каждой итерации бустинга.

диапазон: (0,1]

7)	**colsample_bytree**, **colsample_bylevel**, **colsample_bynode** [по умолчанию=1] (используется только **colsample_bytree**)

Это семейство параметров для подвыборки столбцов.

Все параметры colsample_by* имеют диапазон (0, 1], значение по умолчанию равно 1 и определяют долю столбцов, подлежащих выборке.

colsample_bytree Это коэффициент подвыборки столбцов при построении каждого дерева. Подвыборка выполняется один раз для каждого построенного дерева.

colsample_bylevel Это коэффициент выборки столбцов для каждого уровня. Выборка происходит один раз для каждого нового уровня глубины, достигаемого в дереве. Столбцы выбираются из набора столбцов, выбранных для текущего дерева.

colsample_bynode Это коэффициент подвыборки столбцов для каждого узла (раздела). Подвыборка выполняется каждый раз при оценке нового раздела. Столбцы выбираются из набора столбцов, выбранных для текущего уровня. Это не поддерживается методом точного дерева.

colsample_by* Параметры работают в совокупности. Например, комбинация {'colsample_bytree':0.5, 'colsample_bylevel':0.5, 'colsample_bynode':0.5} с 64 функциями оставит 8 функций для выбора при каждом разделении.

Используя пакет Python или R, можно задать feature_weights для DMatrix, чтобы определить вероятность выбора каждого признака при использовании выборки по столбцам. Аналогичный параметр есть для метода fit в интерфейсе sklearn.

8)	**lambda** [по умолчанию=1, псевдоним: reg_lambda]

Коэффициент регуляризации L2 для весовых коэффициентов. Увеличение этого значения сделает модель более консервативной.

диапазон: [0, ∞]

9)	**alpha** [по умолчанию=0, псевдоним: reg_alpha]

Коэффициент регуляризации L1 для весовых коэффициентов. Увеличение этого значения сделает модель более консервативной.

диапазон: [0, ∞]

10)	**updater** [псевдоним updater]

Строка, разделённая запятыми, определяющая последовательность запускаемых обработчиков деревьев, обеспечивающая модульный способ создания и изменения деревьев. Это расширенный параметр, который обычно устанавливается автоматически в зависимости от других параметров. Однако он также может быть установлен пользователем вручную. Существуют следующие обработчики:

grow_colmaker: построение деревьев на основе нераспределенных столбцов. (! Не используется с методами tree_method = auto или tree_method = hist)

grow_histmaker: построение распределённого дерева с разделением данных по строкам на основе глобального предложения по подсчёту гистограммы. (! Не используется с методами tree_method = auto или tree_method = hist)

grow_quantile_histmaker: Создайте дерево, используя квантованную гистограмму

sync: синхронизирует деревья во всех распределенных узлах.

Допустимые значения для booster = gbtree: grow_colmaker, grow_quantile_histmaker, grow_histmaker или sync (по умолчанию updater = grow_quantile_histmaker).

Для booster = gblinear используются следующие значения гиперпараметра (по умолчанию updater = shotgun):
shotgun: Параллельный алгоритм спуска по координатам, основанный на алгоритме «дробовик». Использует «безумный» параллелизм и поэтому при каждом запуске выдаёт недетерминированное решение. 

Детерминированное решение — это решение, которое выдаёт уникальный и предопределённый результат для заданных входных данных. Такой алгоритм характеризуется чёткой определённостью на каждом шаге и недопустимостью применения методов проб и ошибок. 

Недетерминированное решение — это решение, которое указывает несколько путей обработки одних и тех же входных данных без какого-либо уточнения, какой именно вариант будет выбран. Недетерминированный алгоритм предполагает систематический подход в поиске нужного решения среди всех возможных и основывается на методе проб и ошибок, допуская в своей реализации многократные повторы и случайный выбор. 

Примеры детерминированных алгоритмов: решение математических уравнений, задачи о проверке данных. Примеры недетерминированных алгоритмов: определение всех делителей заданного числа, нахождение нужного слова (или строки) в некотором тексте.
coord_descent: Обычный алгоритм координатного спуска. Также многопоточный, но при этом даёт детерминированное решение. Если параметр device равен cuda или gpu, будет использоваться вариант с графическим процессором.

11)	**tree_method** [используется только для booster = gbtree, для booster = gblinear tree_method = None]

Алгоритм построения дерева, используемый в XGBoost.

Варианты: auto, exact, approx, hist, это комбинация часто используемых обновлений. Для других обновлений, таких как refresh, установите параметр updater напрямую.

auto: То же, что и hist метод дерева.

exact: Точный жадный алгоритм. Перечисляет все кандидаты на разделение.

approx: Приближенный жадный алгоритм с использованием квантильного эскиза и градиентной гистограммы.

hist: Более быстрый приближенный жадный алгоритм, оптимизированный для гистограммы.

**Параметры для booster = gblinear:**

1) **lambda** [по умолчанию=0, псевдоним: reg_lambda]

Нормализация L2 для весовых коэффициентов. Увеличение этого значения сделает модель более консервативной. Нормализовано по количеству обучающих примеров.

2) **alpha** [по умолчанию=0, псевдоним: reg_alpha]

Нормализация L1 для весовых коэффициентов. Увеличение этого значения сделает модель более консервативной. Нормализовано по количеству обучающих примеров.

3) **updater** [по умолчанию= shotgun]

Выбор алгоритма для соответствия линейной модели

shotgun: Параллельный алгоритм спуска по координатам, основанный на алгоритме «shotgun». Использует «безумный» параллелизм и поэтому при каждом запуске выдаёт недетерминированное решение.

coord_descent: Обычный алгоритм координатного спуска. Также многопоточный, но при этом даёт детерминированное решение. Если параметр device равен cuda или gpu, будет использоваться вариант с графическим процессором.

4) **feature_selector** [по умолчанию «None» для booster = gbtree и «cyclic» для booster = gblinear]

Способ выбора наиболее важных или релевантных признаков (features) из набора данных для обучения модели машинного обучения

cyclic: Детерминированный выбор путем циклического перебора функций по одной за раз.

shuffle: Аналогично cyclic но со случайным перемешиванием функций перед каждым обновлением.

random: Случайный (с заменой) выбор координат.

greedy: Выберите координату с наибольшей величиной градиента. Она имеет O(num_feature^2) сложность. Она полностью детерминирована. Она позволяет ограничить выбор до top_k признаков в каждой группе с наибольшей величиной одномерного изменения веса, задав параметр top_k. Это снизит сложность до O(num_feature*top_k).(!Используется только с updater = coord_descent)

thrifty: Экономный, приблизительно жадный выбор признаков. Перед циклическим обновлением упорядочивает признаки в порядке убывания величины их одномерных изменений веса. Эта операция выполняется в нескольких потоках и является линейной аппроксимацией квадратичного жадного выбора. Она позволяет ограничить выбор до top_k признаков в каждой группе с наибольшей величиной одномерного изменения веса, задав параметр top_k. (!Используется только с updater = coord_descent)

5) **top_k** [по умолчанию=0 для feature_selector, принимающего любое значение]

Количество основных функций для выбора в селекторе greedy и thrifty функций. Значение 0 означает использование всех функций. Используется только с updater = «coord_descent»

После расчета гиперпараметров пользователю выводится следующее сообщение с оптимальными значениями гиперпараметров и ошибок (r2_score и MSE):

![image](https://github.com/user-attachments/assets/3fb02ac3-e8ad-4fba-90bc-031963fad9e9)

Значения по умолчанию в данном сообщении также отображаются.

После нажатия кнопки «ОК» пользователь может посмотреть график зависимости лосс функции (r2_score) от кол-ва итераций подбора. Для задачи регрессии важными метриками являются r2_score(точность подбора в %), MSE (среднеквадратическая ошибка в метрах) или Negative MSE (отрицательная среднеквадратическая ошибка в метрах), которая задается при кросс валидации. Выбор между кроссвалидацией и минимизацией метрики r2_score был выполнен в сторону второго. Кроссвалидация выполняется путем разделения объединенной train и test выборки на n фолдов. В каждом фолде вычисляется ошибка, которая суммируется по фолдам и осредняется. При минимизации метрики r2_score используется простой алгоритм уменьшения ошибки на тестовых данных при обучении на train данных. На одних и тех же данных были получены следующие оценки метрики r2_score (вместо Negative MSE использовалась метрика r2). Кроссвалидацией было получено очень низкое значение точности 0.45, по сравнению с обычной минимизацией функции при точности 0.915.

![image](https://github.com/user-attachments/assets/c9eac205-126c-474e-b0b9-b9bdeeb33808)

На данном графике пользователь может отслеживать значение метрики и изменять кол-во требуемых итераций перебора.

На рисунке ниже под графиком лосс функции пользователем может быть применена операция осреднения лосс функции для корректности выполнения процедуры интерпретации. Требуется ввести по скольким точкам будет проведено осреднение графика.

![image](https://github.com/user-attachments/assets/896dbb84-ff29-4d58-a615-c84063f2b136)

# Sklearn RandomForest (scikit learn v.1.4.2):

Random Forest (Случайный лес) — это мощный и популярный алгоритм машинного обучения, который используется как для задач классификации, так и для задач регрессии. Он основан на концепции ансамблевого обучения, где несколько моделей (в данном случае, деревья решений) комбинируются для улучшения предсказаний.

Как работает RandomForest для задачи регрессии:
1. Создание множества деревьев решений:

Random Forest строит множество деревьев решений на основе случайных подмножеств данных. Для каждого дерева используется случайная выборка обучающих данных (с возвращением), что называется методом бутстрэппинга.

При построении каждого дерева также выбирается случайное подмножество признаков (фич), что помогает избежать переобучения и увеличивает разнообразие деревьев.

2.	Обучение деревьев:

Каждое дерево обучается независимо. В процессе обучения каждое дерево пытается минимизировать ошибку, используя критерий, например, среднеквадратичную ошибку (MSE) для регрессии.

Деревья могут иметь разную глубину и структуру, так как они обучаются на различных подмножествах данных и признаков.

3.	Предсказание:

Для выполнения предсказания Random Forest агрегирует предсказания всех деревьев. В случае регрессии это делается путем вычисления среднего значения предсказаний всех деревьев.

4.	Устойчивость и обобщение:

Использование нескольких деревьев и случайных выборок делает Random Forest более устойчивым к шуму и переобучению по сравнению с одиночным деревом решений. Это позволяет алгоритму лучше обобщать данные. Но если данные очень сильно зашумлены, то алгоритм может начать переобучаться.

Методология поиска оптимальных параметров полностью совпадает с предыдущим разделом (XGBoost), поэтому рассмотрим основные гиперпараметры, которые используются в алгоритме. Пользователю предлагается ввести значения следующих гиперпараметров:

![image](https://github.com/user-attachments/assets/341e7cd3-1fdb-43ab-b97c-24c1cafef148)

Гиперпараметры:

1)	**n_estimators** – тип int, по умолчанию=100 

Количество деревьев в лесу. 

2)	**criterion** {“squared_error”, “absolute_error”, “friedman_mse”}, по умолчанию=”squared_error”

Функция для измерения качества разделения. Поддерживаемыми критериями являются “squared_error” для среднеквадратичной ошибки, которая равна уменьшению дисперсии в качестве критерия выбора функции и минимизирует потери в L2, используя среднее значение для каждого конечного узла, “friedman_mse”, который использует среднеквадратичную ошибку с оценкой Фридмана по улучшению для потенциальных разбиений, “absolute_error” для среднего абсолютного значения. ошибка, которая минимизирует потерю L1, используя медиану каждого конечного узла. Обучение с использованием “absolute_error” происходит значительно медленнее, чем при использовании “squared_error”.

3)	**max_depth** – тип int, по умолчанию=None 

Максимальная глубина дерева. Если нет, то узлы расширяются до тех пор, пока все листья не станут чистыми или пока все листья не будут содержать меньше выборок min_samples_split.

4)	**min_samples_split** – тип float, по умолчанию=2 

Минимальное количество выборок, необходимое для разделения внутреннего узла: Если int, то рассматривайте min_samples_split как минимальное число. Если float, то min_samples_split - это дробь, а ceil(min_samples_split * n_samples) - минимальное количество выборок для каждого разделения. 

5)	**min_weight_fraction_leaf** – тип float, по умолчанию=0.0 

Минимальная взвешенная доля от общей суммы весов (всех входных выборок), которая должна быть в конечном узле. Выборки имеют одинаковый вес, если не указано значение sample_weight. 

6)	**max_samples** – тип float, по умолчанию=None 

Если bootstrap имеет значение True (bootstrap = True по умолчанию), то количество выборок, которые нужно отобрать из X для обучения каждого базового оценщика. Если нет (по умолчанию), то нарисуйте образцы X.shape[0]. Если int, то нарисуйте образцы max_samples. Если float, то нарисуйте max((n_samples * max_samples), 1) образцы. Таким образом, значение max_samples должно находиться в интервале (0.0, 1.0].

# Sklearn GBD (GradientBoostingRegressor v.1.4.2)

Градиентный бустинг (для задачи регрессии) — это мощный метод машинного обучения, использующийся для построения предсказательных моделей. Основная идея градиентного бустинга заключается в последовательном добавлении простых моделей (например, деревьев решений), так чтобы каждая последующая модель корректировала ошибки предыдущих.

Как это работает:

1.Инициализация: Модель начинает с простой начальной предсказательной модели, обычно с использованием среднего значения целевой переменной.

2. Пошаговое обучение:
- Для каждой модели в ансамбле вычисляется градиент функции потерь по отношению к предсказаниям текущей композитной модели. Градиент показывает направление наибольшего увеличения ошибки и используется для определения того, как должна измениться следующая модель, чтобы уменьшить ошибку.
- Следующая модель обучается, чтобы предсказывать отрицательный градиент (т.е., ошибку) предыдущих моделей.
- Эта модель затем добавляется к композитной модели с некоторым коэффициентом обучения (learning rate), который определяет степень влияния новой модели на общую модель.

3. Повторение: Этот процесс повторяется множество раз, и каждая новая модель улучшает предсказательные способности композитной модели, пока не будет достигнут критерий остановки или не будет добавлено заранее определенное количество моделей.
Методология поиска оптимальных параметров полностью совпадает с разделом XGBoost, поэтому рассмотрим основные гиперпараметры, которые используются в алгоритме. Пользователю предлагается ввести значения следующих гиперпараметров:

![image](https://github.com/user-attachments/assets/19ba5311-7b72-4403-ab27-4ac06b640dcb)

Гиперпараметры:


1)	**Loss** {‘squared_error’, ‘absolute_error’}, по умолчанию=’squared_error’

Оптимизация функции потери. "squared_error" относится к квадратичной ошибке для регрессии. ‘absolute_error’ относится к абсолютной ошибке регрессии и является надежной функцией потерь.

2)	**learning_rate**, float, по умолчанию=0.1

Скорость обучения уменьшает вклад каждого дерева на величину learning_rate. Существует компромисс между learning_rate и n_estimators. 

Значения должны быть в диапазоне [0.0, inf).
3)	**n_estimatorsint**, по умолчанию =100

Количество выполняемых ступеней усиления. Градиентное усиление достаточно устойчиво к перегрузке, поэтому большое их количество обычно приводит к повышению производительности. 

Значения должны быть в диапазоне [1, inf). 

4)	**subsample**, float, по умолчанию =1.0

Доля выборок, которая будет использоваться для подбора индивидуальных базовых учащихся. Если она меньше 1,0, это приводит к увеличению стохастического градиента. подвыборка взаимодействует с параметром n_estimators. Выбор подвыборки < 1,0 приводит к уменьшению дисперсии и увеличению погрешности. 

Значения должны находиться в диапазоне (0,0, 1,0). 

5)	**criterion** {‘friedman_mse’, ‘squared_error’}, по умолчанию =’friedman_mse’

Функция для измерения качества разделения. Поддерживаемые критерии - “friedman_mse” для среднеквадратичной ошибки с оценкой улучшения по Фридману, “squared_error” для среднеквадратичной ошибки. 

Значение по умолчанию “friedman_mse”, как правило, является наилучшим, поскольку в некоторых случаях оно может обеспечить лучшую аппроксимацию.

6)	**min_samples_split**, float, по умолчанию =0.5

Минимальное количество выборок, необходимое для разделения внутреннего узла: 

Значения должны быть в диапазоне (0.0, 1.0], а значение min_samples_split вычисляется как ceil(min_samples_split * n_samples). 

7)	**min_samples_leaf**, float, по умолчанию =1

Минимальное количество выборок, необходимое для конечного узла. Точка разделения на любой глубине будет учитываться только в том случае, если она оставляет как минимум min_samples_leaf обучающих выборок в каждой левой и правой ветвях. Это может привести к сглаживанию модели, особенно при регрессии. 

Значения должны быть в диапазоне (0.0, 1.0], а значение min_samples_leaf будет равным ceil(min_samples_leaf * n_samples). 

8)	**min_weight_fraction_leaf**, float, по умолчанию=0,0

Минимальная взвешенная доля от общей суммы весов (всех входных выборок), которая должна быть в конечном узле. Выборки имеют одинаковый вес, если не указано значение sample_weight. 

Значения должны быть в диапазоне [0.0, 0.5]. 

9)	**max_depth**, int, по умолчанию =3

Максимальная глубина отдельных регрессионных оценок. Максимальная глубина ограничивает количество узлов в дереве. Настройте этот параметр для достижения наилучшей производительности; наилучшее значение зависит от взаимодействия входных переменных. 

Значения должны быть в диапазоне [1, inf).

10)	**min_impurity_decrease**, float, по умолчанию =0.0

Узел будет разделен, если это разделение приведет к уменьшению примеси больше или равно этому значению. Значения должны быть в диапазоне [0,0, inf). Уравнение уменьшения взвешенных примесей выглядит следующим образом:

N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) 

где N - общее количество выборок, N_t - количество выборок в текущем узле, N_t_L - количество выборок в левом дочернем узле, а N_t_R - количество выборок в правом дочернем узле. N, N_t, N_t_R и N_t_L - все они относятся к взвешенной сумме, если передан параметр sample_weight. 

11)	**max_features** {‘sqrt’, ‘log2’}, int, по умолчанию = 1

Количество признаков, которые следует учитывать при поиске наилучшего разделения: Если int, то значения должны находиться в диапазоне [1, кол-ва признаков).

# Sklearn LinearRegression (scikit learn v.1.4.2):

LinearRegression — это класс из библиотеки scikit-learn, который реализует метод линейной регрессии. Он используется для моделирования зависимости между одной или несколькими независимыми переменными (признаками) и зависимой переменной (целью или целевой переменной). Линейная регрессия находит наилучшие параметры (коэффициенты) для линейной модели, минимизируя сумму квадратов ошибок между предсказанными и фактическими значениями.

LinearRegression принимает на вход следующие параметры:

1)	**fit_intercept** (по умолчанию True): Если True, то интерсепт (свободный член) будет добавлен в модель. Если False, модель будет проходить через начало координат.

2)	**normalize** (по умолчанию False): Если True, входные данные будут нормализованы перед выполнением регрессии. Этот параметр устарел в версии 0.24 и будет удален в будущих версиях. Используйте StandardScaler для нормализации данных.

3)	**copy_X** (по умолчанию True): Если True, входные данные будут скопированы; если False, они могут быть изменены.

4)	**n_jobs** (по умолчанию None): Количество используемых ядер для вычислений. Если -1, используется все доступные ядра.

Все значения гиперпараметров заданы по умолчанию и не требуют корректировок. Гиперпараметр n_jobs использует все доступные ядра и принимает значение (-1).

На выходе пользователь получает только список гиперпараметров модели с значением ошибок r2_score и MSE без графика лосс функции.

![image](https://github.com/user-attachments/assets/0523734e-ce4d-44b9-a3ad-e19950e51155)

# Sklearn Lasso Regression (scikit learn v.1.4.2):

Lasso — это класс из библиотеки scikit-learn, реализующий метод Lasso-регрессии, который является расширением линейной регрессии с L1-регуляризацией. Lasso-регрессия помогает предотвратить переобучение модели, добавляя штраф за сложность модели, что позволяет также выполнять отбор признаков, так как некоторые коэффициенты могут быть сведены к нулю (что позволяет легче интерпретировать модель).

Lasso принимает следующие параметры:

1) **alpha** (по умолчанию 1.0): Параметр регуляризации. Чем больше значение alpha, тем сильнее регуляризация. Значение alpha контролирует, насколько сильно мы хотим уменьшить коэффициенты. Если alpha равно 0, Lasso становится обычной линейной регрессией. Коэффициент alpha выбирается автоматически из следующего диапазона: 0.001, 0.002, 0.005, 0.009, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 1, 2, 3, 5, 8, 10, 20, 50, 100, 500, 1000. При получении минимального значения коэффициента, а именно 0.001, следует обратить внимание на другие гиперпараметры, так как это означает отсутствие сходимости. Подбор гиперпараметра alpha осуществляется методом перекрестной проверки GridSearchCV по заданному выше набору alpha.

2) **fit_intercept** (по умолчанию True): Если True, то интерсепт (свободный член) будет добавлен в модель. Если False, модель будет проходить через начало координат. Данный гиперпараметр задан по умолчанию.
normalize (по умолчанию False): Если True, входные данные будут нормализованы перед выполнением регрессии. Этот параметр устарел в версии 0.24 и будет удален в будущих версиях. Используйте StandardScaler для нормализации данных. Данный гиперпараметр задан по умолчанию.

3) **precompute** (по умолчанию False): Если True, вычисляет матрицу Гессе заранее, что может ускорить процесс, если у вас много признаков. Данный гиперпараметр задан по умолчанию.

4) **max_iter** (по умолчанию 1000): Максимальное количество итераций для алгоритма оптимизации. Если модель не сходит за это количество итераций, могут быть получены неустойчивые результаты.

5) **tol** (по умолчанию 0.0001): Параметр сходимости. Определяет, насколько малым должно быть изменение в значениях коэффициентов для завершения итераций. Если tol слишком велико, алгоритм может завершить работу раньше, чем достигнет оптимального решения. Если tol слишком мало, потребуется больше итераций для достижения сходимости.

**Сходимость tol и max_iter**

При использовании Lasso, важно правильно настроить параметры tol и max_iter. Если значение max_iter установлено слишком маленьким, алгоритм может не успеть сойтись, что приведет к тому, что модель выдаст неустойчивые или неоптимальные результаты. В таких случаях необходимо:

Увеличить значение max_iter (100 000, 200 000 и больше), чтобы дать алгоритму больше времени для достижения сходимости.

Проверить значение tol. Если оно слишком велико, алгоритм может завершить работу до достижения точного решения. Уменьшение tol может помочь улучшить качество модели, но также увеличит время, необходимое для обучения. Желательно ставить гиперпараметр tol в пределах десятитысячных-тысячных.

Lasso регрессия— это мощный инструмент для решения задач линейной регрессии с L1-регуляризацией. Он помогает предотвратить переобучение и может быть использован для отбора признаков. Важно правильно настраивать параметры tol и max_iter, чтобы обеспечить сходимость модели и получить стабильные результаты.

Алгоритм работы:

1)	На входе пользователь вводит значения кол-ва итераций (тип int- целое число) и значение параметра сходимости tol (float – число с плавающей точкой). Если числа не введены, то используются значения по умолчанию.

![image](https://github.com/user-attachments/assets/df744a0c-769f-44bf-b155-e45ded1ce84a)

2)	Далее пользователь получает оптимальное значение коэффициента alpha при заданных им гиперпараметрах, а также значения ошибок r2_score, MSE и Negative MSE (из алгоритма GridSearchCV).

![image](https://github.com/user-attachments/assets/b683576a-8f8c-4e41-a352-fbf7397a4c93)

3)	После нажатия кнопки ОК пользователь получает график зависимости весовых коэффициентов от признаков. В данном случае признаки №4 и 5 вносит существенный вклад в модель, по сравнению с признаками 0,1, 2 и 3. Весовые коэффициенты признака №6 зануляются и он не используется в модели.

![image](https://github.com/user-attachments/assets/5221a914-8491-4118-8ae3-2c5ed1aa60ae)

После этапа подбора гиперпараметров пользователь может перейти в раздел «Визуализация данных», просмотреть и экспортировать свои предсказанные значения.

# Элемент меню – «Прогнозирование»

**Модули fit-predict**

Набор функций fit_predict рассчитывает целевую переменную на прогнозируемом датасете. Пользователь может как ввести значения гиперпараметров (случайные) самостоятельно, так и использовать значения гиперпараметров полученных в разделе Machine Learning, введя их вручную. Все используемые в модулях гиперпараметры полностью совпадают с теми, которые используются в разделе Machine Learning на этапе подбора гиперпараметров.
После ввода гиперпараметров происходит расчет предсказанных значений (с помощью функции predict). Прогнозирование происходит по данным признаков прогнозируемого участка, которые были загружены в функции «Загрузить прогнозную выборку». Обучение происходит по данным train+test выборок, объединенных в один массив. Нормировка данных выполняется как для данных прогнозируемого участка, так и для обучающего датасета методом StandardScaler.
После ввода гиперпараметров на выходе пользователь получает предсказанные значения целевой переменной на прогнозируемом участке при помощи выбранного метода. Например, на рисунке ниже приведен пример применения линейной регрессии к прогнозируемому участку.

![image](https://github.com/user-attachments/assets/a91adce3-470b-40bc-899a-b9ce54bb78c3)

Элементы графического дизайна пользователь может поменять вручную:
- цветовая шкала
- название карты
- минимальное и максимальное значение цветовой палитры данных
- размер точек
- Систему координат
- Перевод из UTM в WGS84 и наоборот

Пользователь может экспортировать предсказанные значения в формате *txt, в котором 1 и 2 столбец являются координатами X и Y соответственно, третий столбец – предсказанные значения. Пример выходного файла:

![image](https://github.com/user-attachments/assets/139ba5a9-1023-4c1c-981d-d0550f3d762c)


