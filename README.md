# GeoGarModel
Software package for geological and geophysical modeling of ground and aerial survey data

# Главное окно программы:
Главное окно программы представлено пространством для визуализации грид файлов регулярной и нерегулярной сети формата *zmap. Пример файла формата *.zmap можно получить путем экспорта грид файла в формат *.zmap в программном обеспечении Surfer 16. Примеры файлов Zmap также представлены в главном дистрибутиве программы.
В правой части располагается список загруженных файлов формата *.zmap. При нажатии на соответствующий файл будет происходить его визуализация в левом окошке со стандартными элементами оформления, а именно:
 - цветовая шкала rainbow
- гистограммное цветовое распределение
- без названия карты
- без подписи цветовой шкалы
Под списком загруженных файлов располагаются элементы изменения оформления карты, которые включают в себя:
- Минимальное значение цветового отображения
- Максимальное значение цветового отображения
- Цветовая шкала:
1) hsv
2) bwr
3) colorwarm
4) gist_rainbow
5) ocean
6) rainbow
7) seismic
8) tab20c
9) terrain
10) twilight
11) twilight_shifted
![image](https://github.com/user-attachments/assets/35c4becd-260b-4b18-a758-65778f58f4cf)
- Название карты
- Подпись цветовой шкалы (справа от цветовой шкалы; rotate = 90)
Под элементами оформления карты находятся 3 кнопки:
1)	«Построить точки обучающей и тестовой выборки». После загрузки обучающего СС датасета, точки обучающей и контрольной (валидационной) выборок можно визуализировать на карте. !Система координат (СК) отображающегося *.zmap файла должна совпадать с СК обучающего датасета для корректной визуализации.
2)	«Добавить shp файл». После загрузки одного или нескольких файлов формата *.shp их можно визуализировать на карте. !Система координат (СК) отображающегося *.zmap файла должна совпадать с СК файла формата *.shp для корректной визуализации.
3)	Кнопка «ОК». При нажатии данной кнопки для уже отображающегося файла формата *.zmap обновляются его элементы оформления. !При этом пункты 1 и 2 полностью удаляются с карты, но сами файлы сохраняются в корне программы и повторная их загрузка не требуется.
# Элементы зума:
В верхней части программы располагается следующая иконка:

 ![image](https://github.com/user-attachments/assets/30b68ec6-c35e-4d2e-b108-f9e7cc04e97d)
 
При нажатии левой кнопкой мыши на данную иконку происходит возврат к стандартному разрешению с возвратом всех элементов, которые были на карте в момент увеличения зума.
Также пользователь может приблизить и отдалить карту. Приближение осуществляется за счет нажатия на следующую кнопку в элементе тулбара:

 ![image](https://github.com/user-attachments/assets/22e015b8-e413-4762-84f5-a1de44880ace)
 
Для отдаления используется иконка:

 ![image](https://github.com/user-attachments/assets/b222a203-e9db-4ff1-8507-b0aaba33518c)
 
Пользователю достаточно нажать 1 раз левой кнопкой мыши в любой области карты для уменьшения масштаба карты на 10% по обоим осям.
# Элемент меню – «Загрузка данных»

1) Загрузить обучающую СС выборку

Данная кнопка реализует загрузку одного файла обучающей выборки (обучающийся участок) формата *.xlsx.
Пример входного файла:

![image](https://github.com/user-attachments/assets/f46fc002-e967-458f-9caf-2e341b388d60)

Первые 2 столбца обязательно должны содержать в себе координаты атрибутов (X, Y).

Далее, начиная с 3 столбца идут признаки, по которым будет происходить обучение.

Кодировка xlsx или xls файла – обязательно UTF8.

Последний столбец является целевой переменной, которая будет предсказываться.

2) Загрузить прогнозную выборку

Данная кнопка реализует загрузку одного файла прогнозной выборки (прогнозный участок) формата *.xlsx.
Файл ничем не отличается от файла обучающей СС выборки за исключением последнего столбца. Для прогнозной выборки целевая переменная неизвестна. Если кол-во столбцов обучающей СС выборки 15, то прогнозной должно быть 14.
- Загрузить shp файл
Данная кнопка реализует загрузку одного или нескольких файлов формата *.shp. !В папке с файлом формата *.shp должны находиться сцепленные с ним файлы следующих форматов:
- *.dbf
- *.prj
- *.sbn
- *.shx

Все названия добавленных *.shp файлов находятся в главном окне программы. При нажатии левой кнопкой мыши пользователь может поменять цвет и толщину линии. Директория расположения shp файла может быть любой (не обязательно в дистрибутиве программы)
Для очистки карты от *.shp файлов достаточно нажать кнопку «ОК» в главном окне программы. Если пользователь захочет визуализировать *.shp файлы снова, он может это сделать также нажав кнопку «Добавить shp файл». Файлы визуализируются с тем же цветом и той же толщиной линии. Пример:

![image](https://github.com/user-attachments/assets/e6cc00c7-c922-428b-a435-0ef482137c52)

3) Загрузить Zmap файл

Данная кнопка реализует загрузку одного или нескольких файлов формата *.zmap. Файлы Zmap должны располагаться в главном дистрибутиве программы, без подпапок.
# Элемент меню – «Просмотр данных»
1) Просмотр данных СС выборки

Данная функция реализует просмотр и редактирование данных обучающей СС выборки. Пример визуализации:

![image](https://github.com/user-attachments/assets/2e78cd49-600a-4a47-8616-34b0a1f2acd2)

2) Просмотр данных прогнозной выборки

Данная функция реализует просмотр и редактирование данных прогнозной выборки.
# Элемент меню – «Визуализация данных»
- Визуализация данных СС выборки

После загрузки СС выборки пользователь может визуализировать её.

- Визуализация данных прогнозной выборки

После загрузки прогнозной выборки пользователь может визуализировать её.

- Визуализация y_pred

После этапа подбора оптимальных гиперпараметров пользователь может визуализировать предсказанные значения (1), данные целевой переменной train выборки (2) и разность предсказанных значений с test выборкой (3). Под графиками располагается 3 кнопки при помощи которых пользователь может экспортировать нужную ему карту в формате *.txt. Первый и второй столбец файла отвечают координатам (X,Y), а третий столбец экспортируемой целевой переменной.

![image](https://github.com/user-attachments/assets/04e6a903-69ac-4885-b9e4-be7063103bf9)

# Элемент меню – «Статистические характеристики данных»
- Статистические характеристики данных СС выборки
- Статистические характеристики данных прогнозной выборки
После загрузки данных обучающей СС выборки пользователь может проанализировать основные статистические хар-ки всех атрибутов, такие как:
- Среднее значение
- Стандартное отклонение
- Минимум
- Максимум
- Квантиль 25%
- Квантиль 75%
Пример:

![image](https://github.com/user-attachments/assets/10dbabd9-96fc-423d-b9ae-6e7bf75d2354)

# Shapely Plot
Математическая основа расчета Shapley значения:
Shapely values в теории игр:
Теория игр - это область математики, изучающей взаимодействие (игру) между игроками, преследующими некие цели и действующими по неким правилам. Кооперативной игрой называется такая игра, в которых группа игроков (коалиция) действует совместно. С середины XX века (Shapley, 1952) известны так называмые Shapley values, которые позволяют численно оценить вклад каждого игрока в достижение общей цели.
Примечание. Понятие "игра" в данном случае может ввести в заблуждение. Обычно под игрой понимается противостояние двух и более сторон, здесь же речь пойдет скорее о кооперативном процессе, в котором каждый участник вносит вклад в общий результат.
Пусть существует характеристическая функция υ, которая каждому множеству игроков сопоставляет число - эффективность данной коалиции игроков, действующей совместно. Тогда Shapley value для каждого игрока - это число, рассчитываемое по достаточно простой формуле. Обозначим за за Δ(i,S) прирост эффективности от добавления игрока i в коалицию игроков S:

![image](https://github.com/user-attachments/assets/b60d5058-e70e-4745-804d-cc2d1d6ce6f8)

Пусть всего есть N игроков. Рассмотрим множество П всех возможных упорядочиваний игроков, и обозначим за (players before i in π) множество игроков, стоящих перед игроков i в упорядочивании π. Shapley value для игрока i рассчитывается таким образом:

![image](https://github.com/user-attachments/assets/edab8c83-1f80-4bbd-b2b9-2e9413a5f619)

То есть мы считаем средний прирост эффективности от добавления i-го игрока в коалицию игроков, стоящих перед ним, по всем возможным упорядочиваниям игроков (количество элементов суммы равно N!).
Формула выше задается аксиоматически, то есть формулируется ряд необходимых свойств и доказывается, что данное решение является единственным, которое им удовлетворяет. Поскольку Δ(i,S) не зависит от порядка игроков в S, то можно объединить равные друг другу слагаемые и переписать формулу в следующем эквивалентном виде:

![image](https://github.com/user-attachments/assets/91e76bdf-1f05-4c46-859e-a6babe3e4331)

Формула является взвешенной суммой по всем подмножествам игроков, не содержащих игрока i, в которой веса принимают наибольшие значения при |S|≈0 или |S|≈ |N| и наименьшие значения при |S|≈ |N|/2.

1) Bar Shapely Plot

При нажатии на кнопку Bar Shapely Plot создает гистограмму из набора значений SHAP для тренировочной выборки.
Пример:

![image](https://github.com/user-attachments/assets/5f8d560a-ef79-4b61-8004-d7fbb13d10a1)

На гистограмме представлены средние значения Shap Value для 13 признаков. Схема читается снизу вверх, и признаки упорядочены по возрастанию их SHAP values. Например, SHAP value для признака Grav_Trend3 (имеющего значение -19.5) говорит о том, что данный атрибут уменьшает величину предсказания модели, по сравнению с отсутствием этого признака, при произвольном наличии других признаков. Таким же образом себя ведет и GravTrend1, Ostat_GravMagInv, GravReg10km, GravReg2km.

2) Beeswarm Shapely Plot

График «basewarm plot» предназначен для отображения подробной информации о том, как основные характеристики набора данных влияют на результаты модели. Каждый экземпляр, для которого дано объяснение, представлен одной точкой на каждой оси. Положение точки по оси X определяется значением SHAP (shap_values.value[instance,feature]) для этой характеристики, и точки «нагромождаются» вдоль каждой строки характеристик, чтобы показать плотность. Цвет используется для отображения исходного значения характеристики (shap_values.data[instance,feature]). На графике ниже мы видим, что пониженные значения региональной составляющей гравитационного поля и глубины границы Мохоровичича имеют повышенное влияние в модели, по сравнению с другими признаками.

![image](https://github.com/user-attachments/assets/ffd9e35b-974c-4b4c-b840-db3ef2ff7ad9)

3) Heatmap Shapely Plot

При передаче матрицы значений SHAP в функцию построения тепловой карты создаётся график с примерами по оси X, входными данными модели по оси Y и значениями SHAP, закодированными в цветовой шкале. По умолчанию образцы упорядочиваются с помощью shap.order.hclust, которая упорядочивает образцы на основе иерархической кластеризации по сходству их объяснений. В результате образцы с одинаковым результатом модели по одной и той же причине группируются вместе.
Выходные данные модели отображаются над матрицей тепловой карты (в центре которой находится .base_value), а глобальная важность каждого входного параметра модели отображается в виде гистограммы в правой части графика (по умолчанию это shap.order.abs.mean показатель общей важности).

![image](https://github.com/user-attachments/assets/c862ad8d-0380-42a8-9aea-120c2aec80d1)
# Элемент меню – «Train_Test»
Данный алгоритм реализует разделение данных обучающего датасета на обучающую и контрольные (тестовая) выборки методом train_test_split из библиотеки scikitlearn. В следующем всплывающем окне пользователь должен ввести число в %, которое будет определять, сколько процентов тестовой выборки использовать:

![image](https://github.com/user-attachments/assets/8a4c3b68-cc09-427b-a930-ed580427b510)

После нажатия кнопки ОК пользователь получит размеры train и test выборок в формате (строки, столбцы):

![image](https://github.com/user-attachments/assets/930895f4-c5d1-44d9-8be8-85788af907e3)

# Элемент меню – «Machine Learning»
Данный элемент меню включает в себя следующие функции:
- Нормализация данных:
Для удаления средних значений и получения единичной дисперсии в данных используется метод StandadScaler из библиотеки scikitlearn.  Данный процесс гарантирует, что все атрибуты будут находятся в одинаковом масштабе, предотвращая доминирование какого-либо атрибута в процессе обучения из-за его большей величины.
Преобразование, выполняемое StandardScaler, можно выразить математически следующим образом:

z = x − μ/ σ

где x представляет собой исходное значение признака, μ — среднее значение признака, σ — стандартное отклонение, а z — стандартизированное значение признака.
После нажатия данной кнопки пользователь получает всплывающее окно, в котором указано, что данные нормализованы.

![image](https://github.com/user-attachments/assets/702a165c-d51e-4399-971b-3d663eb2c7dc)

Нормализация осуществляется как для данных обучающей, так и для данных тестовой выборок.
Метод обучается на тренировочных train данных, к которым применяется функция fit_transform. Для данных test выборки применяется обычная функция transform. Аналогичная методика осуществляется и для получения значений целевой переменной на прогнозном участке. Обучение метода происходит на всех данных обучающего участка, а для прогнозного участка используется метод transform.

# XGBoost (XGBRegressor v.2.1.1):
В данной функции реализован процесс обучения, предсказания и подбора оптимальных гиперпараметров модели градиентного бустинга библиотеки XGBoost на обучающем датасете. При запуске функции пользователю предлагается выбрать диапазоны, в пределах которых будут определяться оптимальные гиперпараметры. Среди самых оптимальных алгоритмов поиска гиперпараметров можно выделить следующие – RandomizedSearchCV (случайный поиск), GridSearchCV (поиск по решетке) и Hyperopt.
Поиск по решётке. В этом способе значения гиперпараметров задаются вручную, затем выполняется их полный перебор. Популярной реализацией этого метода является Grid Search из sklearn. Несмотря на свою простоту этот метод имеет и серьёзные недостатки:
Очень медленный т.к. надо перебрать все комбинации всех параметров. Притом перебор будет продолжаться даже при заведомо неудачных комбинациях.

Часто в целях экономии времени приходится укрупнять шаг перебора, что может привести к тому, что оптимальное значение параметра не будет найдено. Например, если задан диапазон значений от 100 до 1000 с шагом 100 (примером такого параметра может быть количество деревьев в случайном лесе, или градиентном бустинге), а оптимум находится около 550, то GridSearch его не найдёт.

Случайный поиск. Здесь параметры берутся случайным образом из выборки с указанным распределением. В sklearn этот метод реализован как Randomized Search. В большинстве случаев он быстрее GridSearch, к тому же значения параметров не ограничены сеткой. Однако, даже это не всегда позволяет найти оптимум и не защищает от перебора заведомо неудачных комбинаций.

В данной функции, также как и во всех остальных функциях, реализован метод hyperopt – популярной python-библиотеки для подбора гиперпараметров. В ней реализовано 3 алгоритма оптимизации: классический Random Search, метод байесовской оптимизации Tree of Parzen Estimators (TPE), и Simulated Annealing – метод имитации отжига. Hyperopt может работать с разными типами гиперпараметров –непрерывными, дискретными, категориальными и т.д, что является важным преимуществом этой библиотеки. Важным плюсом метода является то, что он сочетает в себе 2 важных качества – это малая времязатратность и отличная точность подобранных гиперпараметров, благодаря применению высокоточных методов оптимизации.

Применение метода hyperopt можно посмотреть в статье по ссылке: https://habr.com/ru/articles/542624/

Окно выбора диапазона изменения гиперпараметров для booster=gbtree выглядит следующим образом:

![image](https://github.com/user-attachments/assets/79749d38-fe84-4e4f-beb7-1f00348eb0cd)

Окно выбора диапазона изменения гиперпараметров для booster=gblinear выглядит следующим образом:

![image](https://github.com/user-attachments/assets/2a1acbc5-7899-4349-b549-d280e1e5cbab)

После нажатия кнопки «ОК» пользователю отображается список пояснений и ошибок:

![image](https://github.com/user-attachments/assets/43204a8d-d506-40a9-aad6-01f587e08052)

Если пользователь ввел интервал, который не укладывается в допустимые пределы, то в окне выше мы получаем ошибку и процедуру следует переделать заново, даже, если на выходе мы получаем график лосс функции. В данном случае для гиперпараметра max_depth был введен интервал от (-1) до 20.

Если пользователь оставил место для ввода гиперпараметра пустым (и минимальное, и максимальное значение, и шаг или максимальное и минимальное значение), тогда в данной консоли выводится сообщение «Пояснение». Стоит обращать внимание только на Ошибки. Если в консоли пользователь увидел ошибки, то ему следует нажать кнопку «ОК» и ввести значения повторно, в противном случае алгоритм не будет запускаться.

Для параметров типа int используется поиск методом hp.quniform = round(uniform(low, high) / q) * q, где q - это целое число определяющее дискретизацию.

Для параметров типа float используется поиск методом hp.uniform.

hp.uniform — этот метод принимает два параметра — нижнее и верхнее значения, определяющие диапазон, из которого мы хотим выбрать различные значения. Он извлекает значения из равномерного распределения.

Для гиперпараметров регуляризации (reg_alpha, reg_lambda) и шага обучения (learning_rate) используется поиск по логнормальному распределению при помощи функции hp.loguniform(‘name_param’, min*np.log(10), max*np.log(10)). Пользователем должны быть введены только числа min и max. Данные числа могут быть как отрицательные и положительные, так и 0. Например, если пользователем было введено min=(-4), а max=2, тогда будет получено логнормальное распределение параметра из диапазона (10^(-4); 10^2) и на выходе в диалоговом окне пользователь получит оптимальное значение гиперпараметра типа float при заданных условиях. Если пользователем ничего не было введено, то используются значения по умолчанию.

Если пользователь не ввел мин. и макс. значение (+шаг) гиперпараметра, то значение гиперпараметра используется по умолчанию. Значения гиперпараметров по умолчанию будет представлено после итогового расчета.
Пользователю предлагается выбрать диапазоны поиска следующих гиперпараметров:

Общие параметры:
1)	**booster [по умолчанию= gbtree]**

Какой усилитель использовать. Можно использовать gbtree или gblinear; gbtree используют модели на основе деревьев, а gblinear использует линейные функции с применением регуляризирующих алгоритмов. Реализация бустера dart будет представлена позже. 

Важным моментом является выбор в качестве бустера gblinear, так как для него используются другие гиперпараметры. Для данного бустера пользователь должен заполнить только диапазоны изменения следующих гиперпараметров – reg_alpha, reg_lambda, updater, feature_selector, top_k. В случае отсутствия ввода одного из данных гиперпараметров, они будут использоваться по умолчанию. Остальные гиперпараметры не будут учитываться при подборе и их значение будет установлено на «по умолчанию».

2)	**device** [по умолчанию= cpu]

Устройство для запуска XGBoost. Пользователь может установить одно из следующих значений:

cpu:Использует центральный процессор.

cuda:Использует графический процессор (устройство CUDA).

Пользователь может ввести либо cpu, либо cuda. При этом, если пользователь хочет использовать версию на графическом процессоре, то ему потребуется наличие поддержки видеокарты технологии CUDA, а также вручную установить на своем ПК версию 2.1.1 XGBoost для графического процессора.

**Параметры для booster = gbtree:**

1)	**eta** [по умолчанию=0.3, псевдоним: learning_rate]

Уменьшение размера шага, используемое при обновлении для предотвращения переобучения. После каждого этапа повышения точности мы можем напрямую получить веса новых признаков, а eta уменьшает веса признаков, чтобы сделать процесс повышения точности более консервативным.

диапазон: [0,1]

2)	**gamma** [по умолчанию=0, псевдоним: min_split_loss]

Минимальное снижение потерь, необходимое для дальнейшего разделения на листовом узле дерева. Чем больше gamma, тем более консервативным будет алгоритм. Обратите внимание, что дерево, в котором не было произведено никаких разделений, всё равно может содержать один конечный узел с ненулевым показателем.

диапазон: [0,∞]

3)	**max_depth** [по умолчанию=6]

Максимальная глубина дерева. Увеличение этого значения сделает модель более сложной и повысит вероятность переобучения. Значение 0 указывает на отсутствие ограничений по глубине. Имейте в виду, что XGBoost активно расходует память при обучении глубокого дерева. exact Метод дерева требует ненулевого значения.

диапазон: [0,∞]

4)	**n_estimators** (тип данных - [int])

Количество деревьев, усиленных градиентом. Эквивалентно количеству раундов усиления.

По умолчанию 1

диапазон: (0,∞]

5)	**min_child_weight** [по умолчанию=1]

Минимальная сумма весов экземпляров (гессиан), необходимых для дочернего элемента. Если на этапе разбиения дерева получается листовой узел с суммой весов экземпляров меньше min_child_weight, то процесс построения прекращает дальнейшее разбиение. В задаче линейной регрессии это просто соответствует минимальному количеству экземпляров, необходимых для каждого узла. Чем больше min_child_weight, тем консервативнее будет алгоритм.

диапазон: [0,∞]

6)	**subsample** [по умолчанию=1]

Коэффициент подвыборки обучающих примеров. Если установить его равным 0,5, XGBoost будет случайным образом выбирать половину обучающих данных перед построением деревьев. Это предотвратит переобучение. Подвыборка будет происходить один раз на каждой итерации бустинга.

диапазон: (0,1]

7)	**colsample_bytree**, **colsample_bylevel**, **colsample_bynode** [по умолчанию=1] (используется только **colsample_bytree**)

Это семейство параметров для подвыборки столбцов.

Все параметры colsample_by* имеют диапазон (0, 1], значение по умолчанию равно 1 и определяют долю столбцов, подлежащих выборке.

colsample_bytree Это коэффициент подвыборки столбцов при построении каждого дерева. Подвыборка выполняется один раз для каждого построенного дерева.

colsample_bylevel Это коэффициент выборки столбцов для каждого уровня. Выборка происходит один раз для каждого нового уровня глубины, достигаемого в дереве. Столбцы выбираются из набора столбцов, выбранных для текущего дерева.

colsample_bynode Это коэффициент подвыборки столбцов для каждого узла (раздела). Подвыборка выполняется каждый раз при оценке нового раздела. Столбцы выбираются из набора столбцов, выбранных для текущего уровня. Это не поддерживается методом точного дерева.

colsample_by* Параметры работают в совокупности. Например, комбинация {'colsample_bytree':0.5, 'colsample_bylevel':0.5, 'colsample_bynode':0.5} с 64 функциями оставит 8 функций для выбора при каждом разделении.

Используя пакет Python или R, можно задать feature_weights для DMatrix, чтобы определить вероятность выбора каждого признака при использовании выборки по столбцам. Аналогичный параметр есть для метода fit в интерфейсе sklearn.

8)	**lambda** [по умолчанию=1, псевдоним: reg_lambda]

Коэффициент регуляризации L2 для весовых коэффициентов. Увеличение этого значения сделает модель более консервативной.

диапазон: [0, ∞]

9)	**alpha** [по умолчанию=0, псевдоним: reg_alpha]

Коэффициент регуляризации L1 для весовых коэффициентов. Увеличение этого значения сделает модель более консервативной.

диапазон: [0, ∞]

10)	**updater** [псевдоним updater]

Строка, разделённая запятыми, определяющая последовательность запускаемых обработчиков деревьев, обеспечивающая модульный способ создания и изменения деревьев. Это расширенный параметр, который обычно устанавливается автоматически в зависимости от других параметров. Однако он также может быть установлен пользователем вручную. Существуют следующие обработчики:

grow_colmaker: построение деревьев на основе нераспределенных столбцов. (! Не используется с методами tree_method = auto или tree_method = hist)

grow_histmaker: построение распределённого дерева с разделением данных по строкам на основе глобального предложения по подсчёту гистограммы. (! Не используется с методами tree_method = auto или tree_method = hist)

grow_quantile_histmaker: Создайте дерево, используя квантованную гистограмму

sync: синхронизирует деревья во всех распределенных узлах.

Допустимые значения для booster = gbtree: grow_colmaker, grow_quantile_histmaker, grow_histmaker или sync (по умолчанию updater = grow_quantile_histmaker).

Для booster = gblinear используются следующие значения гиперпараметра (по умолчанию updater = shotgun):
shotgun: Параллельный алгоритм спуска по координатам, основанный на алгоритме «дробовик». Использует «безумный» параллелизм и поэтому при каждом запуске выдаёт недетерминированное решение. 

Детерминированное решение — это решение, которое выдаёт уникальный и предопределённый результат для заданных входных данных. Такой алгоритм характеризуется чёткой определённостью на каждом шаге и недопустимостью применения методов проб и ошибок. 

Недетерминированное решение — это решение, которое указывает несколько путей обработки одних и тех же входных данных без какого-либо уточнения, какой именно вариант будет выбран. Недетерминированный алгоритм предполагает систематический подход в поиске нужного решения среди всех возможных и основывается на методе проб и ошибок, допуская в своей реализации многократные повторы и случайный выбор. 

Примеры детерминированных алгоритмов: решение математических уравнений, задачи о проверке данных. Примеры недетерминированных алгоритмов: определение всех делителей заданного числа, нахождение нужного слова (или строки) в некотором тексте.
coord_descent: Обычный алгоритм координатного спуска. Также многопоточный, но при этом даёт детерминированное решение. Если параметр device равен cuda или gpu, будет использоваться вариант с графическим процессором.

11)	**tree_method** [используется только для booster = gbtree, для booster = gblinear tree_method = None]

Алгоритм построения дерева, используемый в XGBoost.

Варианты: auto, exact, approx, hist, это комбинация часто используемых обновлений. Для других обновлений, таких как refresh, установите параметр updater напрямую.

auto: То же, что и hist метод дерева.

exact: Точный жадный алгоритм. Перечисляет все кандидаты на разделение.

approx: Приближенный жадный алгоритм с использованием квантильного эскиза и градиентной гистограммы.

hist: Более быстрый приближенный жадный алгоритм, оптимизированный для гистограммы.

**Параметры для booster = gblinear:**

1) **lambda** [по умолчанию=0, псевдоним: reg_lambda]

Нормализация L2 для весовых коэффициентов. Увеличение этого значения сделает модель более консервативной. Нормализовано по количеству обучающих примеров.

2) **alpha** [по умолчанию=0, псевдоним: reg_alpha]

Нормализация L1 для весовых коэффициентов. Увеличение этого значения сделает модель более консервативной. Нормализовано по количеству обучающих примеров.

3) **updater** [по умолчанию= shotgun]

Выбор алгоритма для соответствия линейной модели

shotgun: Параллельный алгоритм спуска по координатам, основанный на алгоритме «shotgun». Использует «безумный» параллелизм и поэтому при каждом запуске выдаёт недетерминированное решение.

coord_descent: Обычный алгоритм координатного спуска. Также многопоточный, но при этом даёт детерминированное решение. Если параметр device равен cuda или gpu, будет использоваться вариант с графическим процессором.

4) **feature_selector** [по умолчанию «None» для booster = gbtree и «cyclic» для booster = gblinear]

Способ выбора наиболее важных или релевантных признаков (features) из набора данных для обучения модели машинного обучения

cyclic: Детерминированный выбор путем циклического перебора функций по одной за раз.

shuffle: Аналогично cyclic но со случайным перемешиванием функций перед каждым обновлением.

random: Случайный (с заменой) выбор координат.

greedy: Выберите координату с наибольшей величиной градиента. Она имеет O(num_feature^2) сложность. Она полностью детерминирована. Она позволяет ограничить выбор до top_k признаков в каждой группе с наибольшей величиной одномерного изменения веса, задав параметр top_k. Это снизит сложность до O(num_feature*top_k).(!Используется только с updater = coord_descent)

thrifty: Экономный, приблизительно жадный выбор признаков. Перед циклическим обновлением упорядочивает признаки в порядке убывания величины их одномерных изменений веса. Эта операция выполняется в нескольких потоках и является линейной аппроксимацией квадратичного жадного выбора. Она позволяет ограничить выбор до top_k признаков в каждой группе с наибольшей величиной одномерного изменения веса, задав параметр top_k. (!Используется только с updater = coord_descent)

5) **top_k** [по умолчанию=0 для feature_selector, принимающего любое значение]

Количество основных функций для выбора в селекторе greedy и thrifty функций. Значение 0 означает использование всех функций. Используется только с updater = «coord_descent»

После расчета гиперпараметров пользователю выводится следующее сообщение с оптимальными значениями гиперпараметров и ошибок (r2_score и MSE):
